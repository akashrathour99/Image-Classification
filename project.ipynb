{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "mount_file_id": "1MiscAdQSWt7L7ms4qgqYH34uCS8J4crO",
      "authorship_tag": "ABX9TyPSsdE/szMMETfJyJ9V68IC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashrathour99/footprint_without_enhencment/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9b2GYtRadyy"
      },
      "outputs": [],
      "source": [
        "!pip install unrar\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzar the file imported from the google drive\n",
        "!unrar x \"/content/drive/MyDrive/afinal_project /Scanned.rar\""
      ],
      "metadata": {
        "id": "U-fS--bqa9Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x \"/content/drive/MyDrive/afinal_project /Foot_print.rar\""
      ],
      "metadata": {
        "id": "ihauJVHIcMYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NbchhpYYcV-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some dependecies for model \n",
        "from skimage.io import imread, imshow\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "#these  2 helps in rotaing the image without loosing the data\n",
        "import argparse\n",
        "import imutils\n"
      ],
      "metadata": {
        "id": "m8UZtIngcWA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x_rcu5bX8rYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this function for spliting training and testing data\n",
        "def check2(img):\n",
        "  if(img[1]=='-'):\n",
        "    return img[2]\n",
        "  elif(img[2]=='-'):\n",
        "    return img[3]\n",
        "\n"
      ],
      "metadata": {
        "id": "vkaUFxKqSDwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this functio gonna help me to get the name of the person for ordering the data to the right file\n",
        "def check(img):\n",
        "  ans=\"\"\n",
        "  ans=img[0]\n",
        "  if(ord(img[1])-48>=0 and ord(img[1])-48<=9):\n",
        "    ans+=img[1]\n",
        "  return ans"
      ],
      "metadata": {
        "id": "XKucI1wSo3fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function gonna help for rotating the images or creating more data for traing\n",
        "#and saving the data to the write folder\n",
        "# the data will be splited in 25% for testing\n",
        "def rot(img1,img,image):\n",
        "  t=1\n",
        "  for angle in np.arange(0, 360, 30):\n",
        "    rotated = imutils.rotate_bound(image, angle)\n",
        "    #this line for storing data for training data\n",
        "    if(check2(img1)=='1'):\n",
        "      path=\"Foot_print/test_data/person\"+check(img)+\"/\"+str(t)+\"_\"+img\n",
        "      cv2.imwrite(path,rotated)\n",
        "    else:\n",
        "      path=\"Foot_print/training_data/person\"+check(img)+\"/\"+str(t)+\"_\"+img\n",
        "      cv2.imwrite(path,rotated)\n",
        "    #this code is for generating testing data\n",
        "    \n",
        "    t=t+1\n"
      ],
      "metadata": {
        "id": "oiZxOXo9pGoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this code gonna help in spliting the scanned data to left and right foot and also changing the data to gray scale\n",
        "\n",
        "\n",
        "left_dir=\"Scanned\"\n",
        "left_items = sorted([x for x in os.listdir(left_dir) if x.endswith(\".jpg\")], key=lambda x: (x.split(\".\")[0]))\n",
        "for img in left_items:\n",
        "  #,as_gray=True\n",
        "  image = imread(\"Scanned\"+\"/\"+img,as_gray=True)\n",
        "  ## this is the gray scale formula\n",
        "  ## imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B \n",
        "  h, w= image.shape\n",
        "  half = w//2\n",
        "  # left foot image\n",
        "  left_part = image[:, :half]    \n",
        "  #right foot image       \n",
        "  right_part = image[:, half:]\n",
        "  # called the rotate function for rotaing the images at specified angle\n",
        "\n",
        "  rot(img,img[:2]+\"-0-\"+img[2:],left_part)\n",
        "  rot(img,img[:2]+\"-1-\"+img[2:],right_part)"
      ],
      "metadata": {
        "id": "CzhhPwnZcMyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "38oBr3o8ckDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifying the folder where images are present\n",
        "TrainingImagePath='Foot_print/training_data'\n",
        " \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True)\n",
        " \n",
        "test_datagen = ImageDataGenerator()\n",
        " \n",
        "# Generating the Training Data\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        TrainingImagePath,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        " \n",
        " \n",
        "# Generating the Testing Data\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        TrainingImagePath,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        " \n",
        "# Printing class labels for each face\n",
        "test_set.class_indices"
      ],
      "metadata": {
        "id": "5W9G815EckK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "TrainClasses=training_set.class_indices\n",
        "\n",
        "\n",
        "ResultMap={}\n",
        "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
        "    ResultMap[faceValue]=faceName\n",
        "\n",
        "# Saving the footprint map for future reference\n",
        "import pickle\n",
        "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
        "    pickle.dump(ResultMap, fileWriteStream)\n",
        "\n",
        "# The model will give answer as a numeric tag\n",
        "# This mapping will help to get the corresponding face name for it\n",
        "print(\"Mapping of Footprint and its ID\",ResultMap)\n",
        "\n",
        "# The number of neurons for the output layer is equal to the number of faces\n",
        "OutputNeurons=len(ResultMap)\n",
        "print('\\n The Number of output neurons: ', OutputNeurons)"
      ],
      "metadata": {
        "id": "oYzxaQwAckOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPool2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n",
        "'''Initializing the Convolutional Neural Network'''\n",
        "classifier= Sequential()\n",
        "\n",
        "''' STEP--1 Convolution\n",
        "# Adding the first layer of CNN\n",
        "# we are using the format (64,64,3) because we are using TensorFlow backend\n",
        "# It means 3 matrix of size (64X64) pixels representing Red, Green and Blue components of pixels\n",
        "'''\n",
        "classifier.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=(64,64,3), activation='relu'))\n",
        "\n",
        "'''# STEP--2 MAX Pooling'''\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "'''############## ADDITIONAL LAYER of CONVOLUTION for better accuracy #################'''\n",
        "classifier.add(Convolution2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
        "\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "'''# STEP--3 FLattening'''\n",
        "classifier.add(Flatten())\n",
        "\n",
        "'''# STEP--4 Fully Connected Neural Network'''\n",
        "classifier.add(Dense(64, activation='relu'))\n",
        "\n",
        "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
        "\n",
        "'''# Compiling the CNN'''\n",
        "#classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
        "\n",
        "###########################################################\n",
        "import time\n",
        "# Measuring the time taken by the model to train\n",
        "StartTime=time.time()\n",
        "\n",
        "# Starting the model training\n",
        "classifier.fit_generator(\n",
        "                    training_set,\n",
        "                    steps_per_epoch=len(training_set),\n",
        "                    epochs=35,\n",
        "                    validation_data=test_set,\n",
        "                    validation_steps=len(test_set))\n",
        "\n",
        "EndTime=time.time()\n",
        "print(\"###### Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes ######')"
      ],
      "metadata": {
        "id": "E5WGPTr0ckZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09655759-3303-4f95-8eb5-12f08169b99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "43/43 [==============================] - 92s 2s/step - loss: 2.8116 - accuracy: 0.1067 - val_loss: 2.5019 - val_accuracy: 0.1411\n",
            "Epoch 2/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 2.2259 - accuracy: 0.2602 - val_loss: 2.3550 - val_accuracy: 0.1615\n",
            "Epoch 3/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 1.5718 - accuracy: 0.4415 - val_loss: 1.2580 - val_accuracy: 0.5607\n",
            "Epoch 4/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 1.1852 - accuracy: 0.5724 - val_loss: 1.6824 - val_accuracy: 0.4525\n",
            "Epoch 5/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.9357 - accuracy: 0.6433 - val_loss: 0.7201 - val_accuracy: 0.7361\n",
            "Epoch 6/35\n",
            "43/43 [==============================] - 91s 2s/step - loss: 0.7264 - accuracy: 0.7266 - val_loss: 0.5583 - val_accuracy: 0.7895\n",
            "Epoch 7/35\n",
            "43/43 [==============================] - 91s 2s/step - loss: 0.5643 - accuracy: 0.8085 - val_loss: 0.7584 - val_accuracy: 0.7368\n",
            "Epoch 8/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.4960 - accuracy: 0.8333 - val_loss: 0.6908 - val_accuracy: 0.7551\n",
            "Epoch 9/35\n",
            "43/43 [==============================] - 103s 2s/step - loss: 0.4337 - accuracy: 0.8341 - val_loss: 0.4942 - val_accuracy: 0.8275\n",
            "Epoch 10/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.3578 - accuracy: 0.8611 - val_loss: 0.3803 - val_accuracy: 0.8421\n",
            "Epoch 11/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.3143 - accuracy: 0.8699 - val_loss: 0.2115 - val_accuracy: 0.9042\n",
            "Epoch 12/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.2837 - accuracy: 0.8816 - val_loss: 0.2781 - val_accuracy: 0.8787\n",
            "Epoch 13/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.2720 - accuracy: 0.8896 - val_loss: 0.1807 - val_accuracy: 0.9174\n",
            "Epoch 14/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.2171 - accuracy: 0.8999 - val_loss: 0.1800 - val_accuracy: 0.9137\n",
            "Epoch 15/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.2455 - accuracy: 0.8889 - val_loss: 0.1463 - val_accuracy: 0.9232\n",
            "Epoch 16/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1962 - accuracy: 0.9123 - val_loss: 0.1546 - val_accuracy: 0.9232\n",
            "Epoch 17/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1821 - accuracy: 0.9108 - val_loss: 0.1652 - val_accuracy: 0.9196\n",
            "Epoch 18/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1775 - accuracy: 0.9145 - val_loss: 0.1643 - val_accuracy: 0.9196\n",
            "Epoch 19/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1761 - accuracy: 0.9101 - val_loss: 0.1337 - val_accuracy: 0.9196\n",
            "Epoch 20/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1559 - accuracy: 0.9181 - val_loss: 0.1265 - val_accuracy: 0.9247\n",
            "Epoch 21/35\n",
            "43/43 [==============================] - 89s 2s/step - loss: 0.1757 - accuracy: 0.9101 - val_loss: 0.1322 - val_accuracy: 0.9225\n",
            "Epoch 22/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1541 - accuracy: 0.9167 - val_loss: 0.1265 - val_accuracy: 0.9269\n",
            "Epoch 23/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1457 - accuracy: 0.9218 - val_loss: 0.1258 - val_accuracy: 0.9284\n",
            "Epoch 24/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1685 - accuracy: 0.9123 - val_loss: 0.1197 - val_accuracy: 0.9240\n",
            "Epoch 25/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1527 - accuracy: 0.9130 - val_loss: 0.1317 - val_accuracy: 0.9232\n",
            "Epoch 26/35\n",
            "43/43 [==============================] - 89s 2s/step - loss: 0.1481 - accuracy: 0.9189 - val_loss: 0.1324 - val_accuracy: 0.9225\n",
            "Epoch 27/35\n",
            "43/43 [==============================] - 89s 2s/step - loss: 0.1736 - accuracy: 0.9057 - val_loss: 0.1242 - val_accuracy: 0.9262\n",
            "Epoch 28/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1589 - accuracy: 0.9225 - val_loss: 0.1239 - val_accuracy: 0.9284\n",
            "Epoch 29/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1562 - accuracy: 0.9079 - val_loss: 0.1240 - val_accuracy: 0.9247\n",
            "Epoch 30/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1471 - accuracy: 0.9101 - val_loss: 0.1134 - val_accuracy: 0.9269\n",
            "Epoch 31/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1315 - accuracy: 0.9225 - val_loss: 0.1162 - val_accuracy: 0.9254\n",
            "Epoch 32/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1215 - accuracy: 0.9203 - val_loss: 0.1334 - val_accuracy: 0.9203\n",
            "Epoch 33/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1182 - accuracy: 0.9189 - val_loss: 0.1097 - val_accuracy: 0.9262\n",
            "Epoch 34/35\n",
            "43/43 [==============================] - 90s 2s/step - loss: 0.1168 - accuracy: 0.9196 - val_loss: 0.1135 - val_accuracy: 0.9291\n",
            "Epoch 35/35\n",
            "43/43 [==============================] - 89s 2s/step - loss: 0.1242 - accuracy: 0.9269 - val_loss: 0.1140 - val_accuracy: 0.9262\n",
            "###### Total Time Taken:  72 Minutes ######\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "ImagePath='/content/Foot_print/test_data/person8/4_8--0-4.jpg'\n",
        "test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
        "test_image=image.img_to_array(test_image)\n",
        "\n",
        "test_image=np.expand_dims(test_image,axis=0)\n",
        "\n",
        "result=classifier.predict(test_image,verbose=0)\n",
        "#print(training_set.class_indices)\n",
        "\n",
        "print('####'*10)\n",
        "print('Prediction is: ',ResultMap[np.argmax(result)])"
      ],
      "metadata": {
        "id": "-TZoVmjWeTYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd318d93-2ab9-418b-d86d-78e16e064559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################################\n",
            "Prediction is:  person8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ibg65ifAj2P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "jSexJ2t3lQiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2e9y2Sl7lQls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E99GmLJ6lQpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5_rwJd0hlQsS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}